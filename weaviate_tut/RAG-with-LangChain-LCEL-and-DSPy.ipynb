{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d24a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "import weaviate\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd327da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = \"\"\n",
    "huggingface_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a42415",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAVIATE_API_KEY = \"\"\n",
    "WEAVIATE_URL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c72b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash = dspy.Google(model=\"gemini-1.5-flash-latest\", api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c37b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\weaviate\\warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Connect to Weaviate Cloud\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url= WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),\n",
    ")\n",
    "\n",
    "print(weaviate_client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd30af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client, k=10)\n",
    "\n",
    "dspy.settings.configure(lm=gemini_flash, rm=retriever_model)\n",
    "gemini_flash(\"say hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f5a82",
   "metadata": {},
   "source": [
    "# Load Dataset (Questions derived from Weaviate's Blog Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5123191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "gold_answers = []\n",
    "queries = []\n",
    "\n",
    "for row in dataset:\n",
    "    gold_answers.append(row[\"gold_answer\"])\n",
    "    queries.append(row[\"query\"])\n",
    "    \n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "trainset, devset, testset = data[:25], data[25:35], data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaebbea",
   "metadata": {},
   "source": [
    "# Metric to Assess Response Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c95891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypedEvaluator(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a system's answer to a question according to a given criterion.\"\"\"\n",
    "    \n",
    "    criterion: str = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    question: str = dspy.InputField(desc=\"The question asked to the system.\")\n",
    "    ground_truth_answer: str = dspy.InputField(desc=\"An expert written Ground Truth Answer to the question.\")\n",
    "    predicted_answer: str = dspy.InputField(desc=\"The system's answer to the question.\")\n",
    "    rating: float = dspy.OutputField(desc=\"A float rating between 1 and 5. IMPORTANT!! ONLY OUTPUT THE RATING!!\")\n",
    "\n",
    "\n",
    "def MetricWrapper(gold, pred, trace=None):\n",
    "    alignment_criterion = \"How aligned is the predicted_answer with the ground_truth?\"\n",
    "    return dspy.TypedPredictor(TypedEvaluator)(criterion=alignment_criterion,\n",
    "                                          question=gold.question,\n",
    "                                          ground_truth_answer=gold.gold_answer,\n",
    "                                          predicted_answer=pred.answer).rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d9fbf",
   "metadata": {},
   "source": [
    "# LCEL RAG Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20969bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\asyncio\\selector_events.py:710: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=4608 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello!\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-308dc6ba-ee9f-4ee2-bf3e-47f4ec2ae1f5-0', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=google_api_key)\n",
    "\n",
    "llm.invoke(\"say hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e18567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From LangChain, import standard modules for prompting.\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Given the context: {context}. What is the answer to the question: `{question}`? IMPORTANT! ONLY OUTPUT THE ANSWER!\")\n",
    "\n",
    "# This is how you'd normally build a chain with LCEL. This chain does retrieval then generation (RAG).\n",
    "retrieve = lambda x: dspy.Retrieve(k=5)(x[\"question\"]).passages\n",
    "vanilla_chain = RunnablePassthrough.assign(context=retrieve) | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fbbcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vanilla_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eef1b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_11352\\4028841941.py:2: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from dspy.predict.langchain import LangChainPredict, LangChainModule\n",
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, underperform, and are about to be deleted. ***\n",
      " \t\tYou are using the client GPT3, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangChainPredict, LangChainModule\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This is how to wrap it so it behaves like a DSPy program.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Just Replace every pattern like `prompt | llm` with `LangChainPredict(prompt, llm)`.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m zeroshot_chain \u001b[38;5;241m=\u001b[39m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(context\u001b[38;5;241m=\u001b[39mretrieve) \u001b[38;5;241m|\u001b[39m \u001b[43mLangChainPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m      7\u001b[0m zeroshot_chain \u001b[38;5;241m=\u001b[39m LangChainModule(zeroshot_chain)  \u001b[38;5;66;03m# then wrap the chain in a DSPy module.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dspy\\predict\\langchain.py:51\u001b[0m, in \u001b[0;36mLangChainPredict.__init__\u001b[1;34m(self, prompt, llm, **config)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m: langchain_template \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mtemplate\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandbytes(\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mhex()\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_field_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlangchain_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dspy\\predict\\langchain.py:79\u001b[0m, in \u001b[0;36mLangChainPredict._build_signature\u001b[1;34m(self, template)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_signature\u001b[39m(\u001b[38;5;28mself\u001b[39m, template):\n\u001b[0;32m     77\u001b[0m     gpt4T \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4-1106-preview\u001b[39m\u001b[38;5;124m'\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mgpt4T): parts \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTemplate2Signature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mstrip(): OldInputField() \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m parts\u001b[38;5;241m.\u001b[39minput_keys\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n\u001b[0;32m     82\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mstrip(): OldOutputField() \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m parts\u001b[38;5;241m.\u001b[39moutput_key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dspy\\utils\\callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[1;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dspy\\predict\\predict.py:93\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dspy\\predict\\predict.py:142\u001b[0m, in \u001b[0;36mPredict.forward\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         completions \u001b[38;5;241m=\u001b[39m new_generate(lm, signature, dsp\u001b[38;5;241m.\u001b[39mExample(demos\u001b[38;5;241m=\u001b[39mdemos, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m         completions \u001b[38;5;241m=\u001b[39m \u001b[43mold_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m pred \u001b[38;5;241m=\u001b[39m Prediction\u001b[38;5;241m.\u001b[39mfrom_completions(completions, signature\u001b[38;5;241m=\u001b[39msignature)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mtrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dspy\\predict\\predict.py:168\u001b[0m, in \u001b[0;36mold_generate\u001b[1;34m(demos, signature, kwargs, config, lm, stage)\u001b[0m\n\u001b[0;32m    165\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\primitives\\predict.py:73\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[1;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[1;32m---> 73\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m generator(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\modules\\gpt3.py:199\u001b[0m, in \u001b[0;36mGPT3.__call__\u001b[1;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m return_sorted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# if kwargs.get(\"n\", 1) > 1:\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m#     if self.model_type == \"chat\":\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m#         kwargs = {**kwargs}\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m#         kwargs = {**kwargs, \"logprobs\": 5}\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_usage(response)\n\u001b[0;32m    202\u001b[0m choices \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\backoff\\_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[0;32m    102\u001b[0m }\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\modules\\gpt3.py:165\u001b[0m, in \u001b[0;36mGPT3.request\u001b[1;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_request(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\modules\\gpt3.py:138\u001b[0m, in \u001b[0;36mGPT3.basic_request\u001b[1;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m messages\n\u001b[0;32m    137\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(kwargs)}\n\u001b[1;32m--> 138\u001b[0m     response \u001b[38;5;241m=\u001b[39m chat_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\modules\\gpt3.py:290\u001b[0m, in \u001b[0;36mchat_request\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m OPENAI_LEGACY:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cached_gpt3_turbo_request_v2_wrapped(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v1_cached_gpt3_turbo_request_v2_wrapped(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\modules\\cache_utils.py:16\u001b[0m, in \u001b[0;36mnoop_decorator.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\modules\\gpt3.py:283\u001b[0m, in \u001b[0;36mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cache_turn_on \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;129m@NotebookCacheMemory\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v1_cached_gpt3_turbo_request_v2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\joblib\\memory.py:655\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\joblib\\memory.py:598\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[1;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[0;32m    595\u001b[0m     must_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_call:\n\u001b[1;32m--> 598\u001b[0m     out, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    600\u001b[0m         \u001b[38;5;66;03m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;66;03m# later calls\u001b[39;00m\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\joblib\\memory.py:856\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28mprint\u001b[39m(format_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, args, kwargs))\n\u001b[1;32m--> 856\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_backend\u001b[38;5;241m.\u001b[39mdump_item(\n\u001b[0;32m    858\u001b[0m     [func_id, args_id], output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[0;32m    860\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\dsp\\modules\\gpt3.py:277\u001b[0m, in \u001b[0;36mv1_cached_gpt3_turbo_request_v2\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    276\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\openai\\_utils\\_proxy.py:20\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\openai\\_utils\\_proxy.py:55\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\openai\\_module_client.py:12\u001b[0m, in \u001b[0;36mChatProxy.__load__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resources\u001b[38;5;241m.\u001b[39mChat:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchat\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\openai\\__init__.py:327\u001b[0m, in \u001b[0;36m_load_client\u001b[1;34m()\u001b[0m\n\u001b[0;32m    311\u001b[0m         _client \u001b[38;5;241m=\u001b[39m _AzureModuleClient(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    312\u001b[0m             api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[0;32m    313\u001b[0m             azure_endpoint\u001b[38;5;241m=\u001b[39mazure_endpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    323\u001b[0m             http_client\u001b[38;5;241m=\u001b[39mhttp_client,\n\u001b[0;32m    324\u001b[0m         )\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[1;32m--> 327\u001b[0m     _client \u001b[38;5;241m=\u001b[39m \u001b[43m_ModuleClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\openai\\_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# From DSPy, import the modules that know how to interact with LangChain LCEL.\n",
    "from dspy.predict.langchain import LangChainPredict, LangChainModule\n",
    "\n",
    "# This is how to wrap it so it behaves like a DSPy program.\n",
    "# Just Replace every pattern like `prompt | llm` with `LangChainPredict(prompt, llm)`.\n",
    "zeroshot_chain = RunnablePassthrough.assign(context=retrieve) | LangChainPredict(prompt, llm) | StrOutputParser()\n",
    "zeroshot_chain = LangChainModule(zeroshot_chain)  # then wrap the chain in a DSPy module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c01986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dspy.predict.langchain.LangChainModule"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zeroshot_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9090cfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "You are a processor for prompts. I will give you a prompt template (Python f-string) for an arbitrary task for other LMs.\n",
      "Your job is to prepare three modular pieces: (i) any essential task instructions or guidelines, (ii) a list of variable names for inputs, (iv) the variable name for output.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Template:\n",
      "```\n",
      "\n",
      "${template}\n",
      "\n",
      "```\n",
      "\n",
      "Let's now prepare three modular pieces.\n",
      "\n",
      "Essential Instructions: ${essential_instructions}\n",
      "\n",
      "Input Keys: comma-separated list of valid variable names\n",
      "\n",
      "Output Key: a valid variable name\n",
      "\n",
      "---\n",
      "\n",
      "Template:\n",
      "```\n",
      "\n",
      "Given the context: {context}. What is the answer to the question: `{question}`? IMPORTANT! ONLY OUTPUT THE ANSWER!\n",
      "\n",
      "```\n",
      "\n",
      "Let's now prepare three modular pieces.\n",
      "\n",
      "Essential Instructions:\u001b[32mEssential Instructions: The context should be a string containing relevant information. The question should be a string asking a question about the context. The output should be a string containing only the answer to the question.\n",
      "\n",
      "Input Keys: context, question\n",
      "\n",
      "Output Key: answer \n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nYou are a processor for prompts. I will give you a prompt template (Python f-string) for an arbitrary task for other LMs.\\nYour job is to prepare three modular pieces: (i) any essential task instructions or guidelines, (ii) a list of variable names for inputs, (iv) the variable name for output.\\n\\n---\\n\\nFollow the following format.\\n\\nTemplate:\\n```\\n\\n${template}\\n\\n```\\n\\nLet's now prepare three modular pieces.\\n\\nEssential Instructions: ${essential_instructions}\\n\\nInput Keys: comma-separated list of valid variable names\\n\\nOutput Key: a valid variable name\\n\\n---\\n\\nTemplate:\\n```\\n\\nGiven the context: {context}. What is the answer to the question: `{question}`? IMPORTANT! ONLY OUTPUT THE ANSWER!\\n\\n```\\n\\nLet's now prepare three modular pieces.\\n\\nEssential Instructions:\\x1b[32mEssential Instructions: The context should be a string containing relevant information. The question should be a string asking a question about the context. The output should be a string containing only the answer to the question.\\n\\nInput Keys: context, question\\n\\nOutput Key: answer \\n\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_flash.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a7b08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Lock Striping is a pattern that solves race conditions, that can occur during parallel data imports, by using a fixed number of locks. It assigns objects to a specific lock based on their UUID, ensuring that objects with the same UUID are not processed concurrently while allowing objects with different UUIDs to be processed in parallel. \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Lock Striping?\"\n",
    "\n",
    "zeroshot_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa6b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▎                                      | 3/25 [00:09<01:12,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "optimizer = BootstrapFewShot(metric=MetricWrapper,\n",
    "                            max_bootstrapped_demos=3)\n",
    "\n",
    "optimized_chain = optimizer.compile(zeroshot_chain, teacher=zeroshot_chain, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cd72879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Lock Striping is a solution for race conditions in database design, particularly when importing data in parallel streams. It uses a fixed number of locks to ensure objects with the same UUID are never processed concurrently, preventing data duplication without sacrificing import performance. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9c97e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dspy.predict.langchain.LangChainModule"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimized_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4314019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimized_chain.chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
